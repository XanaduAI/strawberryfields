{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nGraph similarity\n================\n\n*Technical details are available in the API documentation:* :doc:`/code/api/strawberryfields.apps.similarity`\n\nThis page looks at how to use GBS to construct a similarity measure between graphs,\nknown as a graph kernel :cite:`schuld2019quantum`. Kernels can be applied to graph-based\ndata for machine learning tasks such as classification using a support vector machine.\n\nGraph data\n----------\n\nWe begin by fixing a dataset of graphs to consider and loading GBS samples from these graphs,\nwhich will be needed in the following.\n\nLet's use the MUTAG dataset of graphs :cite:`debnath1991structure,kriege2012subgraph`. This is a\ndataset of 188 different graphs that each correspond to the structure of a chemical compound. Our\ngoal is to use GBS samples from these graphs to measure their similarity.\n\nThe :mod:`~.apps.data` module provides pre-calculated GBS samples for selected graphs in the MUTAG\ndataset. Each set of samples is generated by encoding the graph into a GBS device, and collecting\nphoton click events. We'll start by loading four sets of samples and visualizing the\ncorresponding graphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from strawberryfields.apps import data, plot, similarity\n\nm0 = data.Mutag0()\nm1 = data.Mutag1()\nm2 = data.Mutag2()\nm3 = data.Mutag3()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These datasets contain both the adjacency matrix of the graph and the samples generated through\nGBS. We can access the adjacency matrix through:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m0_a = m0.adj\nm1_a = m1.adj\nm2_a = m2.adj\nm3_a = m3.adj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Samples from these graphs can be accessed by indexing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(m0[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot the four graphs using the :mod:`~.apps.plot` module. To use this module,\nwe need to convert the adjacency matrices into NetworkX Graphs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import networkx as nx\nimport plotly\n\nplot_mutag_0 = plot.graph(nx.Graph(m0_a))\nplot_mutag_1 = plot.graph(nx.Graph(m1_a))\nplot_mutag_2 = plot.graph(nx.Graph(m2_a))\nplot_mutag_3 = plot.graph(nx.Graph(m3_a))\n\nplotly.offline.plot(plot_mutag_0, filename=\"MUTAG_0.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n    :file: ../../examples_apps/MUTAG_0.html\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The command ``plotly.offline.plot()`` is used to display plots in the documentation. In\n    practice, you can simply use ``plot_mutag_0.show()`` to view your graph.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotly.offline.plot(plot_mutag_1, filename=\"MUTAG_1.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n    :file: ../../examples_apps/MUTAG_1.html\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotly.offline.plot(plot_mutag_2, filename=\"MUTAG_2.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n    :file: ../../examples_apps/MUTAG_2.html\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotly.offline.plot(plot_mutag_3, filename=\"MUTAG_3.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n    :file: ../../examples_apps/MUTAG_3.html\n\nThe graphs of ``m1_a`` and ``m2_a`` look very similar. In fact,\nit turns out that they are *isomorphic* to each other, which means that the graphs can be made\nidentical by permuting their node labels.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a feature vector\n-------------------------\n\nFollowing :cite:`schuld2019quantum`, we can create a *feature vector* to describe each graph.\nThese feature vectors contain information about the graphs and can be viewed as a mapping to a\nhigh-dimensional feature space, a technique often used in machine learning that allows us to\nemploy properties of the feature space to separate and classify the vectors.\n\nThe feature vector of a graph can be composed in a variety of ways. One approach is to\nassociate features with the relative frequencies of certain types of measurements being\nrecorded from a GBS device configured to sample from the graph, as we now discuss.\n\nWe begin by defining the concept of an *orbit*, which is the set of all GBS samples that are\nequivalent under permutation of the modes. A sample can be converted to its corresponding orbit\nusing the :func:`~.sample_to_orbit` function. For example, the first sample of ``m0`` is ``[0,\n0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]`` and has orbit:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.sample_to_orbit(m0[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, ``[1, 1]`` means that two photons were detected, each in a separate mode. Other samples\ncan be randomly generated from the ``[1, 1]`` orbit using:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.orbit_to_sample([1, 1], modes=m0.modes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Orbits provide a useful way to coarse-grain the samples from GBS into outcomes that are\nstatistically more likely to be observed. However, we are interested in coarse-graining further\ninto *events*, which correspond to a combination of orbits with the same photon number such\nthat the number of photons counted in each mode does not exceed a fixed value\n``max_count_per_mode``. To understand this, let's look at all of the orbits with a photon\nnumber of 5:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(list(similarity.orbits(5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All 5-photon samples belong to one of the orbits above. A 5-photon event with\n``max_count_per_mode = 3`` means that we include the orbits: ``[[1, 1, 1, 1, 1], [2, 1, 1, 1],\n[3, 1, 1], [2, 2, 1], [3, 2]]`` and ignore the orbits ``[[4, 1], [5]]``. For example,\nthe sample ``[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0]`` is a 5-photon event:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.sample_to_event([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Samples with more than ``max_count_per_mode`` in any mode are not counted as part of the event:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.sample_to_event([0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have mastered orbits and events, how can we make a feature vector? It was shown in\n:cite:`schuld2019quantum` that one way of making a feature vector of a graph is through the\nfrequencies of events. Specifically, for a $k$ photon event $E_{k, n_{\\max}}$\nwith maximum count per mode $n_{\\max}$ and corresponding probability $p_{k,\nn_{\\max}}:=p_{E_{k, n_{\\max}}}(G)$ with respect to a graph $G$, a feature vector can be\nwritten as\n\n\\begin{align}f_{\\mathbf{k}, n_{\\max}} = (p_{k_{1}, n_{\\max}}, p_{k_{2}, n_{\\max}}, \\ldots , p_{k_{K},\n        n_{\\max}}),\\end{align}\n\nwhere $\\mathbf{k} := (k_{1}, k_{2}, \\ldots , k_{K})$ is a list of different total photon\nnumbers.\n\nFor example, if $\\mathbf{k} := (2, 4, 6)$ and $n_{\\max} = 2$, we have\n\n\\begin{align}f_{(2, 4, 6), 2} = (p_{2, 2}, p_{4, 2}, p_{6, 2}).\\end{align}\n\nIn this case, we are interested in the probabilities of events $E_{2, 2}$, $E_{4,\n2}$, and $E_{6, 2}$. Suppose we are sampling from a four-mode device and have the samples\n``[0, 3, 0, 1]`` and ``[1, 2, 0, 1]``. These samples are part of the orbits ``[3, 1]`` and\n``[2, 1, 1]``, respectively. However, ``[3, 1]`` is not part of the $E_{4, 2}$ event while\n``[2, 1, 1]`` is.\n\nCalculating a feature vector\n----------------------------\n\nWe provide two methods for calculating a feature vector of GBS event probabilities in\nStrawberry Fields:\n\n1. Through sampling.\n2. Using a Monte Carlo estimate of the probability.\n\nIn the first method, all one needs to do is generate some GBS samples from the graph of\ninterest and fix the composition of the feature vector. For example, for a feature vector\n$f_{\\mathbf{k} = (2, 4, 6), n_{\\max}=2}$ we use:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.feature_vector_sampling(m0, event_photon_numbers=[2, 4, 6], max_count_per_mode=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the second method, suppose we want to calculate the event probabilities exactly rather than\nthrough sampling. To do this, we consider the event probability $p_{k, n_{\\max}}$ as the\nsum over all sample probabilities in the event. In GBS, each sample probability is determined by\nthe hafnian of a relevant sub-adjacency matrix. While this is tough to calculate, what makes\ncalculating $p_{k, n_{\\max}}$ really challenging is the number of samples the corresponding\nevent contains! For example, the 6-photon event over 17 modes $E_{k=6, n_{\\max}=2}$\ncontains the following number of samples :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.event_cardinality(6, 2, 17))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To avoid calculating a large number of sample probabilities, an alternative is to perform a\nMonte Carlo approximation. Here, samples within an event are selected uniformly at random and\ntheir resultant probabilities are calculated. If $N$ samples $\\{S_{1}, S_{2},\n\\ldots , S_{N}\\}$ are generated, then the event probability can be approximated as\n\n\\begin{align}p(E_{k, n_{\\max}}) \\approx \\frac{1}{N}\\sum_{i=1}^N p(S_i) |E_{k, n_{\\max}}|,\\end{align}\n\nwith $|E_{k, n_{\\max}}|$ denoting the cardinality of the event.\n\nThis method can be accessed using the :func:`~.prob_event_mc` function. The 4-photon event is\napproximated as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(similarity.prob_event_mc(nx.Graph(m0_a), 4, max_count_per_mode=2, n_mean=6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feature vector can then be calculated through Monte Carlo sampling using\n:func:`~.feature_vector_mc`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The results of :func:`~.prob_event_mc` and :func:`~.feature_vector_mc` are probabilistic and\n    may vary between runs. Increasing the optional ``samples`` parameter will increase accuracy\n    but slow down calculation.</p></div>\n\nThe second method of Monte Carlo approximation is intended for use in scenarios where it is\ncomputationally intensive to pre-calculate a statistically significant dataset of samples from\nGBS.\n\nMachine learning with GBS graph kernels\n---------------------------------------\n\nThe power of feature vectors that embed graphs in a vector space of real numbers is that we can\nnow measure similarities between graphs. This is very useful in machine learning, where similar\nlabels are assigned to graphs that are close to each other. GBS feature vectors therefore give\nrise to a similarity measure between graphs!\n\nLet's build this up a bit more. The MUTAG dataset we are considering contains not only graphs\ncorresponding to the structure of chemical compounds, but also a *label* of each\ncompound based upon its mutagenic effect. The four graphs we consider here have labels:\n\n- MUTAG0: Class 1\n- MUTAG1: Class 0\n- MUTAG2: Class 0\n- MUTAG3: Class 1\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classes = [1, 0, 0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use GBS feature vectors in a `support vector machine\n<https://en.wikipedia.org/wiki/Support-vector_machine>`__ (SVM) that finds a separating\nhyperplane between classes in the feature space. We start by defining two-dimensional feature\nvectors:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events = [8, 10]\nmax_count = 2\n\nf1 = similarity.feature_vector_sampling(m0, events, max_count)\nf2 = similarity.feature_vector_sampling(m1, events, max_count)\nf3 = similarity.feature_vector_sampling(m2, events, max_count)\nf4 = similarity.feature_vector_sampling(m3, events, max_count)\n\nimport numpy as np\n\nR = np.array([f1, f2, f3, f4])\n\nprint(R)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is freedom in the choice of ``events`` composing the feature vectors and we encourage the\nreader to explore different combinations. Note, however, that odd photon-numbered events have\nzero probability because ideal GBS only generates and outputs pairs of photons.\n\nGiven our points in the feature space and their target labels, we can use\nscikit-learn's Support Vector Machine `LinearSVC <https://scikit-learn.org/stable/modules/generated/sklearn.svm\n.LinearSVC.html>`__ as our model to train:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\n\nR_scaled = StandardScaler().fit_transform(R)  # Transform data to zero mean and unit variance\n\nclassifier = LinearSVC()\nclassifier.fit(R_scaled, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the term \"linear\" refers to the *kernel* function used to calculate inner products\nbetween vectors in the space. We can use a linear SVM because we have already embedded the\ngraphs in a feature space based on GBS. We have also rescaled the feature vectors so that they\nzero mean and unit variance using scikit-learn's ``StandardScaler``, a technique\n`often used <https://scikit-learn.org/stable/modules/preprocessing.html>`__ in machine learning.\n\nWe can then visualize the trained SVM by plotting the decision boundary with respect to the\npoints:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "w = classifier.coef_[0]\ni = classifier.intercept_[0]\n\nm = -w[0] / w[1]  # finding the values for y = mx + b\nb = -i / w[1]\n\nxx = [-1, 1]\nyy = [m * x + b for x in xx]\n\nfig = plot.points(R_scaled, classes)\nfig.add_trace(plotly.graph_objects.Scatter(x=xx, y=yy, mode=\"lines\"))\n\nplotly.offline.plot(fig, filename=\"SVM.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. raw:: html\n    :file: ../../examples_apps/SVM.html\n\nThis plot shows the two classes (grey points for class 0 and red points for class 1)\nsuccessfully separated by the linear hyperplane using the GBS feature space. Moreover,\nrecall that the two MUTAG1 and MUTAG2 graphs of class 0 are actually isomorphic. Reassuringly,\ntheir corresponding feature vectors are very similar. In fact, the feature vectors of\nisomorphic graphs should always be identical :cite:`bradler2018graph` - the small discrepancy\nin this plot is due to the statistical approximation from sampling.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}